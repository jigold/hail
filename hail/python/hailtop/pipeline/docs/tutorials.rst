.. _sec-tutorial:

========
Tutorial
========

This tutorial goes through the basic concepts of Pipeline with examples.


Overview
--------

A :class:`.Pipeline` consists of a set of :class:`.Task`s to execute. There can be
an arbitrary number of tasks in the pipeline that are executed in order of their dependencies.
A dependency between two tasks states that the dependent task should not run until
the previous task completes. Thus, under the covers a pipeline is a directed acyclic graph (DAG)
of tasks.

# Image here of an example of a DAG

Import
------

Pipeline is located inside the `hailtop` module, which can be installed
as described in the :ref:`Getting Started <sec-getting_started>` section.

.. code-block:: python

    >>> import hailtop.pipeline as hp


.. _f-strings:

f-strings
---------

f-strings were added to Python in version 3.6 and are denoted by the 'f' character
before a string literal. When creating the string, Python evaluates any expressions
in single curly braces `{...}` using the current variable scope. When Python compiles
the example below, the string 'Alice' is substituted for `{name}` because the variable
`name` is set to 'Alice' in the line above.

.. code-block:: python

    >>> name = 'Alice'
    >>> print(f'hello {name}')
    'hello Alice'

You can put any arbitrary Python code inside the curly braces and Python will evaluate
the expression correctly. For example, below we evaluate `x + 1` first before compiling
the string. Therefore, we get 'x = 6' as the resulting string.

.. code-block:: python

    >>> x = 5
    >>> print(f'x = {x + 1}')
    'x = 6'

To use an f-string and output a single curly brace in the output string, escape the curly
brace by duplicating the character. For example, `{` becomes `{{` in the string definition,
but will print as `{`. Likewise, `}` becomes `}}`, but will print as `}`.

.. code-block:: python

    >>> x = 5
    >>> print(f'x = {{x + 1}} plus {x}')
    'x = {x + 1} plus 5'

To learn more about f-strings, check out this `tutorial <https://www.datacamp.com/community/tutorials/f-string-formatting-in-python>`_.

Hello World
-----------

In the example below, we have defined a :class:`.Pipeline` `p` with the name 'hello'.
We use the method :meth:`.Pipeline.new_task` to create a task object which we call `t` and then
use the method :meth:`.Task.command` to tell Pipeline that we want to execute `echo "hello world"`.
However, at this point, Pipeline hasn't actually run the task to print "hello world". All we have
done is specified the tasks and the order in which they should be run. To actually execute the
Pipeline, we call :meth:`.Pipeline.run`. The `name` arguments to both :class:`.Pipeline` and
:class:`.Task` are used in the :ref:`Batch Service UI <batch-service>`.

.. code-block:: python

    >>> p = hp.Pipeline(name='hello')
    >>> t = p.new_task(name='t1')
    >>> t.command('echo "hello world"')
    >>> p.run()


Now that we know how to create a pipeline with a single task, we call :meth:`.Pipeline.new_task`
twice to create two tasks `s` and `t` which both will print a variant of hello world to stdout.
Calling `p.run()` executes the pipeline. By default, pipelines are executed by the :class:`.LocalBackend`
which runs tasks on your local computer. Therefore, even though these tasks can be run in parallel,
they are still run sequentially. However, if pipelines are executed by the :class:`.BatchBackend`
using the :ref:`Batch Service <sec-batch_service>`, then `s` and `t` can be run in parallel as
there exist no dependencies between them.

.. code-block:: python

    >>> p = hp.Pipeline(name='hello-parallel')
    >>> s = p.new_task(name='t1')
    >>> s.command('echo "hello world 1"')
    >>> t = p.new_task(name='t2')
    >>> t.command('echo "hello world 2"')
    >>> p.run()

To create a dependency between `s` and `t`, we use the method :class:`.Task.depends_on` to
explicitly state that `t` depends on `s`. In both the :class:`.LocalBackend` and
:class:`.BatchBackend`, `s` will always run before `t`.

.. code-block:: python

    >>> p = hp.Pipeline(name='hello-serial')
    >>> s = p.new_task(name='t1')
    >>> s.command('echo "hello world 1"')
    >>> t = p.new_task(name='t2')
    >>> t.command('echo "hello world 2"')
    >>> t.depends_on(s)
    >>> p.run()


.. _file-dependencies:

File Dependencies
-----------------

So far we have created pipelines with two tasks where the dependencies between
them were declared explicitly. However, in many pipelines, we want to have a file
generated by one task be the input to a downstream task. Pipeline has a mechanism
for tracking file outputs and then inferring task dependencies from the usage of
those files.

In the example below, we have specified two tasks: `s` and `t`. `s` prints
"hello world" as in previous examples. However, instead of printing to stdout,
this time `s` redirects the output to a temporary file defined by `s.ofile`.
`s.ofile` is a Python object of type :class:`.TaskResourceFile` that was created
on the fly when we accessed an attribute of a :class:`.Task` that does not already
exist. Any time we access the attribute again (in this example `ofile`), we get the
same :class:`.TaskResourceFile` that was previously created. However, be aware that
you cannot use an existing method or property name of :class:`.Task` objects such
as :meth:`.Task.command` or :meth:`.Task.image`.

Note the 'f' character before the string in the command for `s`! We placed `s.ofile` in curly braces so
when Python interpolates the :ref:`f-string <f-string>`, it replaced the
:class:`.TaskResourceFile` object with an actual file path into the command for `s`.
We use another f-string in `t`'s command where we print the contents of `s.ofile` to stdout.
`s.ofile` is the same temporary file that was created in the command for `t`. Therefore,
pipeline deduces that `t` must depend on `s` and thus creates an implicit dependency for `t` on `s`.
In both the :class:`.LocalBackend` and :class:`.BatchBackend`, `s` will always run before `t`.


.. code-block:: python

    >>> p = hp.Pipeline(name='hello-serial')
    >>> s = p.new_task(name='t1')
    >>> s.command(f'echo "hello world" > {s.ofile}')
    >>> t = p.new_task(name='t2')
    >>> t.command(f'cat {s.ofile}')
    >>> p.run()


Scatter / Gather
----------------

Pipeline is implemented in Python making it easy to use for loops
to create more complicated dependency graphs between tasks. We define a scatter
to be a pipeline that runs the same command with varying input parameters and a gather
is a final task or "sink" that waits for all of the tasks in the scatter to be complete
before executing.

# insert picture of scatter/gather dag

In the example below, we use a for loop to create a task for each one of
'Alice', 'Bob', and 'Dan' that prints the name of the user programatically
thereby scattering the echo command over users.

.. code-block:: python

    >>> p = hp.Pipeline(name='scatter')
    >>> for name in ['Alice', 'Bob', 'Dan']:
    ...     t = p.new_task(name=name)
    ...     t.command(f'echo "hello {name}"')
    >>> p.run()

In the previous example, we did not assign the tasks we created for each
user to a unique variable name and instead named it `t` each time in the
for loop. However, if we want to add a final gather task (`sink`) that depends on the
completion of all user tasks, then we need to keep track of all of the user
tasks so we can use the :meth:`.Task.depends_on` method to explicitly link
the `sink` task to be dependent on the user tasks, which are stored in the
`tasks` array. The single asterisk before `tasks` is used in Python to have
all elements in the array be treated as separate input arguments to the function,
in this case :meth:`.Task.depends_on`.


.. code-block:: python

    >>> p = hp.Pipeline(name='scatter-gather-1')
    >>> tasks = []
    >>> for name in ['Alice', 'Bob', 'Dan']:
    ...     t = p.new_task(name=name)
    ...     t.command(f'echo "hello {name}"')
    ...     tasks.append(t)
    >>> sink = p.new_task(name='sink')
    >>> sink.depends_on(*tasks)
    >>> p.run()

Now that we know how to create a `sink` task that depends on an arbitrary
number of tasks, we want to have the outputs of each of the per-user tasks
be implicit file dependencies in the `sink` task (see the section on
:ref:`file dependencies <file-dependencies>`). The changes from the previous
example to make this happen are each task `t` uses an :ref:`f-string <f-string>`
to create a temporary output file `t.ofile` where the output to echo is redirected.
We then use all of the output files in the `sink` command by creating a string
with the temporary output file names for each task. A :class:`.TaskResourceFile`
is a Pipeline-specific object that inherits from `str`. Therefore, you can use
:class:`.TaskResourceFile`s as if they were strings, which we do with the `join`
command for strings.


.. code-block:: python

    >>> p = hp.Pipeline(name='scatter-gather-2')
    >>> tasks = []
    >>> for name in ['Alice', 'Bob', 'Dan']:
    ...     t = p.new_task(name=name)
    ...     t.command(f'echo "hello {name}" > {t.ofile}')
    ...     tasks.append(t)
    >>> sink = p.new_task(name='sink')
    >>> sink.command('cat {}'.format(' '.join([t.ofile for t in tasks]))
    >>> p.run()


Nested Scatters
---------------

We can also create a nested scatter where we do a series of tasks per user.
This is equivalent to a nested for loop. In the example below, we instantiate a
new :class:`.Pipeline` object `p`. Then for each user in 'Alice', 'Bob', and 'Dan'
we create new tasks for making the bed, doing laundry, and grocery shopping. In total,
we will have created 9 tasks that run in parallel as we did not define any dependencies
between the tasks.

.. code-block:: python

    >>> p = hp.Pipeline(name='nested-scatter-1')
    >>> for user in ['Alice', 'Bob', 'Dan']:
    ...     for chore in ['make-bed', 'laundry', 'grocery-shop']:
    ...         t = p.new_task(name=f'{user}-{chore}')
    ...         t.command(f'echo "user {user} is doing chore {chore}"')
    >>> p.run()


We can implement the same example as above with a function that implements the inner
for loop. The `do_chores` function takes a :class:`.Pipeline` object to add new tasks
to and a user name for whom to create chore tasks for. Like above, we create 9 independent
tasks. However, by structuring the code into smaller functions that take pipeline objects,
we can create more complicated dependency graphs and reuse components across various pipelines.


.. code-block:: python

    >>> def do_chores(p, user):
    ...     for chore in ['make-bed', 'laundry', 'grocery-shop']:
    ...         t = p.new_task(name=f'{user}-{chore}')
    ...         t.command(f'echo "user {user} is doing chore {chore}"')

    >>> p = hp.Pipeline(name='nested-scatter-2')
    >>> for user in ['Alice', 'Bob', 'Dan']:
    ...     do_chores(p, user)
    >>> p.run()

Lastly, we provide an example of a more complicated pipeline that has an initial
task, then scatters tasks per user, then has a series of gather / sink tasks
to wait for the per user tasks to be done before completing the pipeline.

.. code-block:: python

    >>> def do_chores(p, head, user):
    ...     chores = []
    ...     for chore in ['make-bed', 'laundry', 'grocery-shop']:
    ...         t = p.new_task(name=f'{user}-{chore}')
    ...         t.command(f'echo "user {user} is doing chore {chore}"')
    ...         t.depends_on(head)
    ...         chores.append(t)
    ...     sink = p.new_task(name=f'{user}-sink')
    ...     sink.depends_on(*chores)
    ...     return sink

    >>> p = hp.Pipeline(name='nested-scatter-3')
    >>> head = p.new_task(name='head')
    >>> user_sinks = []
    >>> for user in ['Alice', 'Bob', 'Dan']:
    ...     user_sink = do_chores(p, head, user)
    ...     user_sinks.append(user_sink)
    >>> final_sink = p.new_task(name='final-sink')
    >>> final_sink.depends_on(*user_sinks)
    >>> p.run()


Input Files
-----------



.. code-block:: python

    >>> p = hp.Pipeline(name='hello-input')
    >>> input = p.read_input('data/hello.txt')
    >>> t = p.new_task(name='hello')
    >>> t.command('cat {input}')
    >>> p.run()


Output Files
------------

.. code-block:: python

    >>> p = hp.Pipeline(name='hello-input')
    >>> t = p.new_task(name='hello')
    >>> t.command('echo "hello" > {t.ofile}')
    >>> p.write_output(t.ofile, 'output/hello.txt')
    >>> p.run()


Resource Groups
---------------

.. code-block:: python

    >>> p = hp.Pipeline(name='resource-groups')
    >>> bfile = p.read_input_group(bed='data/example.bed',
    ...                            bim='data/example.bim',
    ...                            fam='data/example.fam')
    >>> wc_bim = p.new_task(name='wc-bim')
    >>> wc_bim.command(f'wc -l {bfile.bim}')
    >>> wc_fam = p.new_task(name='wc-fam')
    >>> wc_fam.command(f'wc -l {bfile.fam}')
    >>> p.run()


.. code-block:: python

    >>> p = hp.Pipeline(name='resource-groups')
    >>> create = p.new_task(name='create-dummy')
    >>> create.declare_resource_group(bfile={'bed': '{root}.bed',
    ...                                      'bim': '{root}.bim',
    ...                                      'fam': '{root}.fam'}
    >>> create.command(f'plink --dummy 10 100 --make-bed --out {create.bfile}')
    >>> p.run()
