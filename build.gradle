buildscript {
    repositories {
        mavenCentral()
        jcenter()
    }
}

plugins {
  id "com.gradle.build-scan" version "1.0"
  id 'java'
  id 'scala'
  id 'idea'
  id 'application'
  id 'maven'
  id 'jacoco'
  id 'com.github.johnrengelman.shadow' version '1.2.3'
}

import com.github.jengelman.gradle.plugins.shadow.tasks.ShadowJar

repositories {
    // Get kudu-spark2_2.11 with custom pom from our repo since we can't
    // figure out how to enable spark2_2.11 profile in standard pom.
    maven {
        url 'https://storage.googleapis.com/hail-common/maven-repo'
    }
    mavenCentral()
    jcenter()
    maven {
        url "https://repository.cloudera.com/artifactory/cloudera-repos/" // kudu
    }
}

buildScan {
    licenseAgreementUrl = 'https://gradle.com/terms-of-service'
    licenseAgree = 'yes'
}

mainClassName = "org.broadinstitute.hail.driver.Main"

String sparkVersion = System.getProperty("spark.version","2.0.2")

Boolean spark2 = sparkVersion ==~ /^2.*/

String mongoVersion
String kuduSpark
String scalaVersion
String scalaMajorVersion
String py4jVersion
String breezeVersion = '0.11.2'

if (spark2) {
    kuduSpark = 'spark2'
    scalaVersion = '2.11.8'
    scalaMajorVersion = '2.11'
    py4jVersion = '0.10.3'
    mongoVersion = '2.0.0'
} else {
    scalaVersion = '2.10.4'
    scalaMajorVersion = '2.10'
    kuduSpark = 'spark'
    if (sparkVersion ==~ /^1\.5.*/) {
        py4jVersion = '0.8.2.1'
        mongoVersion = '1.0.0'
    } else {
        py4jVersion = '0.9' // 1.6
        mongoVersion = '1.1.0'
    }
}

String sparkHome = System.getProperty("spark.home", System.env.SPARK_HOME)

sourceSets.main.scala.srcDir "src/main/java"
sourceSets.main.java.srcDirs = []

compileJava {
    options.compilerArgs << "-Xlint:all" << "-Werror"
}

compileScala {
    if (spark2)
        // until we drop Spark 1 support, we need to rely on deprecated features
        scalaCompileOptions.additionalParameters = ["-feature"]
    else
        scalaCompileOptions.additionalParameters = ["-feature", "-Xfatal-warnings"]
}

if (!spark2) {
    configurations.all {
        resolutionStrategy {
            // override new version pulled in by solrj
            // http://stackoverflow.com/questions/31039367/spark-parallelize-could-not-find-creator-property-with-name-id

            force 'com.fasterxml.jackson.core:jackson-databind:2.4.4'
        }
    }
}

dependencies {
    compile 'org.scala-lang:scala-library:' + scalaVersion
    compile 'org.scala-lang:scala-reflect:' + scalaVersion
    compile('org.apache.spark:spark-core_' + scalaMajorVersion + ':' + sparkVersion) {
        exclude module: 'hadoop-client'
    }
    compile('org.apache.hadoop:hadoop-client:2.7.1') {
        exclude module: 'servlet-api'
    }
    compile 'org.apache.spark:spark-sql_' + scalaMajorVersion + ':' + sparkVersion
    compile 'org.apache.spark:spark-mllib_' + scalaMajorVersion + ':' + sparkVersion
    compile 'net.jpountz.lz4:lz4:1.3.0'
    compile 'org.scalanlp:breeze-natives_' + scalaMajorVersion + ':' + breezeVersion
    compile 'args4j:args4j:2.32'
    compile 'com.github.samtools:htsjdk:2.5.0'
    compile 'org.apache.kudu:kudu-client:1.1.0'
    compile 'org.apache.kudu:kudu-' + kuduSpark + '_' + scalaMajorVersion + ':1.1.0'

    compile 'org.http4s:http4s-core_' + scalaMajorVersion + ':0.12.3'
    compile 'org.http4s:http4s-server_' + scalaMajorVersion + ':0.12.3'
    compile 'org.http4s:http4s-argonaut_' + scalaMajorVersion + ':0.12.3'
    compile 'org.http4s:http4s-dsl_' + scalaMajorVersion + ':0.12.3'
    compile 'org.http4s:http4s-scala-xml_' + scalaMajorVersion + ':0.12.3'
    compile 'org.http4s:http4s-client_' + scalaMajorVersion + ':0.12.3'
    compile 'org.http4s:http4s-websocket_' + scalaMajorVersion + ':0.1.3'
    compile 'org.http4s:http4s-blaze-core_' + scalaMajorVersion + ':0.12.3'
    compile 'org.http4s:http4s-blaze-client_' + scalaMajorVersion + ':0.12.3'
    compile 'org.http4s:http4s-blaze-server_' + scalaMajorVersion + ':0.12.3'
    compile 'org.json4s:json4s-core_' + scalaMajorVersion + ':3.2.10'
    // compile 'org.json4s:json4s-native_' + scalaMajorVersion + ':3.2.10'
    compile 'org.json4s:json4s-jackson_' + scalaMajorVersion + ':3.2.10'
    compile 'org.json4s:json4s-ast_' + scalaMajorVersion + ':3.2.10'
    //compile 'org.json4s:json4s-native_' + scalaMajorVersion + ':3.3.0'
    //compile 'org.json4s:json4s-jackson_' + scalaMajorVersion + ':3.3.0'
    //compile 'org.json4s:json4s-ast_' + scalaMajorVersion + ':3.3.0'

    compile 'org.mongodb.spark:mongo-spark-connector_' + scalaMajorVersion + ':' + mongoVersion

    compile 'org.apache.solr:solr-solrj:6.2.0'
    compile 'com.datastax.cassandra:cassandra-driver-core:3.0.0'

    compile 'com.jayway.restassured:rest-assured:2.8.0'

    compile group: 'org.ow2.asm', name: 'asm', version: '5.1'
    compile group: 'org.ow2.asm', name: 'asm-util', version: '5.1'
    compile group: 'org.ow2.asm', name: 'asm-analysis', version: '5.1'

    compile 'net.sourceforge.jdistlib:jdistlib:0.4.5'

    testCompile 'org.testng:testng:6.8.21'
    testCompile 'org.scalatest:scalatest_' + scalaMajorVersion + ':2.2.4'
}

task(checkSettings) << {
    def checkSeed = System.getProperty("check.seed", "1")
    if (checkSeed == "random")
        checkSeed = new Random().nextInt().toString()
    def checkSize = System.getProperty("check.size", "1000")
    def checkCount = System.getProperty("check.count", "10")

    println "check: seed = $checkSeed, size = $checkSize, count = $checkCount"

    // override with these defaults, random seed
    System.setProperty("check.seed", checkSeed)
    System.setProperty("check.size", checkSize)
    System.setProperty("check.count", checkCount)
}

test {
    useTestNG {}

    systemProperties System.getProperties()

    testLogging {
        events "passed", "skipped", "failed"
    }

    // listen to events in the test execution lifecycle
    beforeTest { descriptor ->
        logger.lifecycle("Running test: " + descriptor)
    }
}

test.dependsOn(checkSettings)

task testPyHail(type: Exec, dependsOn: shadowJar) {
     commandLine 'python', '-m', 'unittest', 'pyhail.tests'

     environment SPARK_HOME: sparkHome
     environment PYTHONPATH: '' + projectDir + '/python:' + sparkHome + '/python:' + sparkHome + '/python/lib/py4j-' + py4jVersion + '-src.zip'
     environment SPARK_CLASSPATH: '' + projectDir + '/build/libs/hail-all-spark.jar'
}

test.dependsOn(testPyHail)

tasks.withType(ShadowJar) {
    manifest {
        attributes 'Implementation-Title': 'Hail',
                'Implementation-Version': '0.0.1-SNAPSHOT',
                'Main-Class': 'org.broadinstitute.hail.driver.Main'
    }
    baseName = project.name + '-all'
    mergeServiceFiles()
    zip64 true
    // conflict with version in default Hadoop/Spark install
    relocate 'org.apache.http', 'org.broadinstitute.hail.relocated.org.apache.http'
    relocate 'com.google.common', 'org.broadinstitute.hail.relocated.com.google.common'
    relocate 'org.objectweb', 'org.broadinstitute.hail.relocated.org.objectweb'
}

shadowJar {
    classifier = 'spark'
    from(project.sourceSets.main.output)
    configurations = [project.configurations.runtime]
    dependencies {
        include(dependency('net.jpountz.lz4:lz4:.*'))
        include(dependency('org.scalanlp:breeze-natives_' + scalaMajorVersion + ':.*'))
        include(dependency('args4j:args4j:.*'))
        include(dependency('com.github.samtools:htsjdk:.*'))

        include(dependency('org.json4s:json4s-core_' + scalaMajorVersion + ':.*'))
        // include(dependency('org.json4s:json4s-native_' + scalaMajorVersion + ':.*'))
        include(dependency('org.json4s:json4s-jackson_' + scalaMajorVersion + ':.*'))
        include(dependency('org.json4s:json4s-ast_' + scalaMajorVersion + ':.*'))

        include(dependency('org.http4s:http4s-core_' + scalaMajorVersion + ':.*'))

        include(dependency('org.apache.kudu:kudu-client:.*'))
        include(dependency('org.apache.kudu:kudu-' + kuduSpark + '_' + scalaMajorVersion + ':.*'))

        include(dependency('org.mongodb.spark:mongo-spark-connector_' + scalaMajorVersion + ':.*'))
        include(dependency('org.mongodb:mongo-java-driver:.*'))

        // solr dependencies
        include(dependency('org.apache.solr:solr-solrj:.*'))
        include(dependency('org.apache.httpcomponents:httpclient:.*'))
        include(dependency('org.apache.httpcomponents:httpcore:.*'))
        include(dependency('org.apache.httpcomponents:httpmime:.*'))
        include(dependency('org.apache.zookeeper:zookeeper:.*'))
        include(dependency('org.codehaus.woodstox:stax2-api:.*'))
        include(dependency('org.codehaus.woodstox:woodstox-core-asl:.*'))
        include(dependency('org.noggit:noggit:.*'))

        include(dependency('com.datastax.cassandra:cassandra-driver-core:.*'))
        include(dependency('com.google.guava:guava:.*'))
        include(dependency('org.apache.spark:kudu-client:.*'))
        include(dependency('org.apache.spark:kudu-spark_' + scalaMajorVersion + ':.*'))

        include(dependency('org.ow2.asm:asm:.*'))
        include(dependency('org.ow2.asm:asm-util:.*'))
        include(dependency('org.ow2.asm:asm-tree:.*'))
        include(dependency('org.ow2.asm:asm-analysis:.*'))

        include(dependency('net.sourceforge.jdistlib:jdistlib:.*'))
    }
}

task shadowTestJar(type: ShadowJar) {
    classifier = 'spark-test'
    from(project.sourceSets.main.output, project.sourceSets.test.output)
    configurations = [project.configurations.testRuntime]
    dependencies {
        include(dependency('net.jpountz.lz4:lz4:.*'))
        include(dependency('org.scalanlp:breeze-natives_' + scalaMajorVersion + ':.*'))
        include(dependency('args4j:args4j:.*'))
        include(dependency('com.github.samtools:htsjdk:.*'))

        include(dependency('org.json4s:json4s-core_' + scalaMajorVersion + ':.*'))
        // include(dependency('org.json4s:json4s-native_' + scalaMajorVersion + ':.*'))
        include(dependency('org.json4s:json4s-jackson_' + scalaMajorVersion + ':.*'))
        include(dependency('org.json4s:json4s-ast_' + scalaMajorVersion + ':.*'))

        include(dependency('org.http4s:http4s-core_' + scalaMajorVersion + ':.*'))
        include(dependency('org.scalaz:scalaz-core_' + scalaMajorVersion + ':.*'))
        include(dependency('org.scalaz:scalaz-concurrent_' + scalaMajorVersion + ':.*'))
        include(dependency('org.scalaz:scalaz-stream_' + scalaMajorVersion + ':.*'))

        include(dependency('org.apache.kudu:kudu-client:.*'))
        include(dependency('org.apache.kudu:kudu-' + kuduSpark + '_' + scalaMajorVersion + ':.*'))

        include(dependency('org.mongodb.spark:mongo-spark-connector_' + scalaMajorVersion + ':.*'))
        include(dependency('org.mongodb:mongo-java-driver:.*'))

        // solr dependencies
        include(dependency('org.apache.solr:solr-solrj:.*'))
        include(dependency('org.apache.httpcomponents:httpclient:.*'))
        include(dependency('org.apache.httpcomponents:httpcore:.*'))
        include(dependency('org.apache.httpcomponents:httpmime:.*'))
        include(dependency('org.apache.zookeeper:zookeeper:.*'))
        include(dependency('org.codehaus.woodstox:stax2-api:.*'))
        include(dependency('org.codehaus.woodstox:woodstox-core-asl:.*'))
        include(dependency('org.noggit:noggit:.*'))

        include(dependency('com.datastax.cassandra:cassandra-driver-core:.*'))
        include(dependency('com.google.guava:guava:.*'))
        include(dependency('org.apache.spark:kudu-client:.*'))
        include(dependency('org.apache.spark:kudu-spark_' + scalaMajorVersion + ':.*'))

        include(dependency('org.ow2.asm:asm:.*'))
        include(dependency('org.ow2.asm:asm-util:.*'))
        include(dependency('org.ow2.asm:asm-tree:.*'))
        include(dependency('org.ow2.asm:asm-analysis:.*'))

        include(dependency('org.testng:testng:.*'))
        include(dependency('com.beust:jcommander:.*'))
        include(dependency('org.scalatest:scalatest_' + scalaMajorVersion + ':.*'))

        include(dependency('net.sourceforge.jdistlib:jdistlib:.*'))
    }
}

jacocoTestReport {
    dependsOn test
    reports {
        xml.enabled false
        csv.enabled false
        html.destination "${buildDir}/reports/coverage"
    }
}

task coverage(dependsOn: jacocoTestReport)

task testJar(type: Jar) {
    classifier = 'tests'
    from sourceSets.test.output
}

installDist {
  into('python') {
    from 'python'
    include '**/*.py'
  }
}

task wrapper(type: Wrapper) {
    gradleVersion = '2.14.1'
}

task setupDocsDirs(type: Exec) {
    commandLine 'mkdir', '-p', 'build/tmp/docs/', 'build/www/', 'build/tmp/tutorial/'
}

task copyHtml(type: Copy, dependsOn: setupDocsDirs) {
    from 'docs/html'
    into 'build/tmp/docs'
}

task copyJavascript(type: Copy, dependsOn: setupDocsDirs) {
    from 'docs/javascript'
    into 'build/tmp/docs'
}

task copyTutorialImages(type: Copy, dependsOn: setupDocsDirs) {
    from 'docs/tutorial/'
    into 'build/www'
    include '**/*.png'
}

task copyTex(type: Copy, dependsOn: setupDocsDirs) {
    from 'docs/LeveneHaldane.tex'
    into 'build/www'
}

task copyWebsiteContent(type: Copy, dependsOn: setupDocsDirs) {
    from 'www/'
    into 'build/www'
}

task runPandoc(type: Exec, dependsOn: setupDocsDirs) {
    args('docs', 'build/tmp/docs/')
    executable 'src/test/resources/runPandoc.sh'
}

task runSass(type: Exec) {
    args('python/pyhail/docs/_static/navbar.scss', 'python/pyhail/docs/_static/navbar.css')
    executable 'sass'
}

task makeTutorial(type: Exec, dependsOn: setupDocsDirs) {
    args('docs/tutorial/Tutorial.md','docs/tutorial/', 'build/tmp/tutorial/', 'build/www/')
    executable 'docs/tutorial/packageTutorial.sh'
}

task runCommandsJson(type: JavaExec, dependsOn: ['setupDocsDirs', 'classes']) {
    main 'org.broadinstitute.hail.driver.Main'
    args ('commandmeta', '-o', 'build/tmp/docs/commandOptions.json')
    classpath sourceSets.main.runtimeClasspath
}

task makeFunctionsRst(type: JavaExec, dependsOn: ['setupDocsDirs', 'classes']) {
    main 'org.broadinstitute.hail.driver.Main'
    args ('docfunctions', '-o', 'python/pyhail/docs/functions.rst')
    classpath sourceSets.main.runtimeClasspath
}

task prepareJavascript(type: Exec, dependsOn: 'copyJavascript') {
    workingDir 'build/tmp/docs'
    commandLine 'npm', 'install'


    standardOutput = new ByteArrayOutputStream()
    errorOutput = standardOutput
    ignoreExitValue = true
    doLast {
        if (execResult.exitValue != 0) {
            println(standardOutput.toString())
            throw new GradleException("exec failed; see output above")
        }
    }
}

task runNodeDocs(type: Exec, dependsOn: ['runPandoc', 'runCommandsJson', 'prepareJavascript', 'copyHtml']) {
    args('referenceTemplate.html', 'commandsTemplate.html', 'faqTemplate.html', 'template.html', './commandOptions.json', 'docs/')
    executable 'build/tmp/docs/compileDocsNode.js'
}

task makePyHailDocs(type: Exec, dependsOn: ['runSass', 'makeFunctionsRst']) {
     workingDir 'python/pyhail/docs'

     commandLine 'make', 'html'

     environment PYTHONPATH: '' + projectDir + '/python:' + sparkHome + '/python:' + sparkHome + '/python/lib/py4j-' + py4jVersion + '-src.zip'
}

task copyPyHailDocs(type: Exec, dependsOn: ['makePyHailDocs', 'setupDocsDirs']) {
    commandLine 'mv', 'python/pyhail/docs/_build/html', 'build/www/pyhail'
}

task copyCompiledDocs(type: Copy, dependsOn: ['runNodeDocs']) {
    from 'build/tmp/docs/faq.html'
    from 'build/tmp/docs/commands.html'
    from 'build/tmp/docs/reference.html'
    from 'build/tmp/docs/tutorial.html'
    from 'build/tmp/docs/overview.html'
    from 'build/tmp/docs/getting_started.html'
    from 'build/tmp/docs/index.html'
    into 'build/www'
}

task createWebsite(dependsOn: ['copyTex', 'copyWebsiteContent', 'copyCompiledDocs', 'copyTutorialImages', 'makeTutorial', 'copyPyHailDocs']) {
}

task createDocs(dependsOn: createWebsite)
